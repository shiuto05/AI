1. シンプルなベースラインの作成
(1) ルールベースのベースライン
最も簡単な方法として、「全員を生存したと予測する」または「女性は生存、男性は非生存と予測する」といったルールベースの予測を立てます。

python
コピーする
編集する
import pandas as pd

# データの読み込み
train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")

# すべての乗客を「生存」と予測
test['Survived'] = 1

# 「女性は生存、男性は非生存」という単純なルール
test.loc[test['Sex'] == 'male', 'Survived'] = 0

# 提出ファイルを作成
test[['PassengerId', 'Survived']].to_csv("submission_baseline.csv", index=False)
この単純なルールでも 約77% の精度 が出ます（Kaggleのスコア換算）。

(2) シンプルな機械学習モデル
次に、シンプルなモデル（ロジスティック回帰）を使ってベースラインを作成します。

python
コピーする
編集する
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# データの前処理
train['Sex'] = train['Sex'].map({'male': 0, 'female': 1})
train = train[['Sex', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Survived']].dropna()

X = train.drop('Survived', axis=1)
y = train['Survived']

# データを学習用と評価用に分割
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)

# モデルの訓練
model = LogisticRegression()
model.fit(X_train, y_train)

# 予測と評価
y_pred = model.predict(X_valid)
print("Accuracy:", accuracy_score(y_valid, y_pred))
この段階で 約78〜80% の精度 になることが多いです。

2. ベースラインを改善してKaggle上位を狙う
ここからがKaggleで上位に入るための戦略です。

(1) 特徴量エンジニアリング
欠損値の補完: Age や Fare の欠損を中央値や平均値で補完する
新しい特徴量を作る:
FamilySize = SibSp + Parch + 1（家族の人数）
IsAlone = 1 if FamilySize == 1 else 0（一人旅かどうか）
名前の敬称（Mr., Miss., Dr. など）を抽出して特徴量にする
python
コピーする
編集する
# 家族の人数
train['FamilySize'] = train['SibSp'] + train['Parch'] + 1
train['IsAlone'] = (train['FamilySize'] == 1).astype(int)

# 名前から敬称を抽出
train['Title'] = train['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)
train['Title'] = train['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 
                                         'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')
train['Title'] = train['Title'].replace(['Mlle', 'Ms'], 'Miss')
train['Title'] = train['Title'].replace('Mme', 'Mrs')
train['Title'] = train['Title'].map({'Mr': 0, 'Miss': 1, 'Mrs': 2, 'Master': 3, 'Rare': 4})
train['Title'] = train['Title'].fillna(0)
このように特徴量を工夫すると、スコアが 81〜83% に向上します。

(2) モデルの高度化
ベースラインのロジスティック回帰から、より強力なモデルに変更します。

ランダムフォレスト (RandomForestClassifier)
XGBoost (XGBClassifier)
LightGBM (LGBMClassifier)
python
コピーする
編集する
from xgboost import XGBClassifier

model = XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_valid)

print("Accuracy:", accuracy_score(y_valid, y_pred))
XGBoostやLightGBMを使うことで、 85%前後のスコア を出せることが多く、上位10%に入る可能性が高くなります。

(3) ハイパーパラメータチューニング
Kaggle上位を狙うなら、ハイパーパラメータチューニングは必須です。

Grid Search や Random Search で最適なパラメータを探す
Optuna などのライブラリを使って自動最適化
python
コピーする
編集する
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2]
}

grid_search = GridSearchCV(XGBClassifier(), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

print("Best parameters:", grid_search.best_params_)
適切なパラメータを見つけると、スコアが 87〜89% まで向上し、上位5%に入ることも可能です。

(4) アンサンブル学習
最後に、複数のモデルを組み合わせることでスコアをさらに向上させます。

平均値を取る（単純アンサンブル）
VotingClassifier を使う（多数決）
Stacking（メタモデルを使う）
python
コピーする
編集する
from sklearn.ensemble import VotingClassifier

model1 = XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1)
model2 = RandomForestClassifier(n_estimators=200, max_depth=5)
model3 = LogisticRegression()

ensemble = VotingClassifier(estimators=[
    ('xgb', model1),
    ('rf', model2),
    ('lr', model3)
], voting='soft')

ensemble.fit(X_train, y_train)
y_pred = ensemble.predict(X_valid)

print("Accuracy:", accuracy_score(y_valid, y_pred))
アンサンブルを適用すると、スコアが 90% 近く まで向上することもあります。

まとめ
最初にルールベースのベースラインを作る
ロジスティック回帰などのシンプルなモデルを試す
特徴量エンジニアリングを行い、精度を上げる
XGBoost / LightGBM などの強力なモデルを適用
ハイパーパラメータを最適化する
アンサンブル学習でスコアをさらに向上
この流れを意識すると、Kaggleの上位10%に入れる可能性が高くなります。